{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cultural project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is dedicated to the application of machine learning algorithms and tools to analysis of anthropological and sociological datasets. We would experiment with both supervised and unsupervised learning algorithms, such as clustering, decision trees, association rules, and artificial neural networks and many others. The goal is to extract meaningful patterns from such datasets as World Value Survey.\n",
    "\n",
    "Before we apply any algorithms to the WVS data, we need to make sure that the data has high quality. We need to detect and correct corrupt, invalid, or inaccurate records from the data set and changing the range of data value for easiler processing later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_0.csv: data with headers(the title of questions)  \n",
    "data_1.csv: data with several most representative country in each cluster, and missing values removed/imputed & columns combined  \n",
    "data_2.csv: data with religion code transformed and ready to use\n",
    "\n",
    "----------------------------------------------------\n",
    "data_3.csv: normalized data & convert categorical into numeric for clustering  \n",
    "data_4.csv: data with labels from clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = \"../data_set/cultural_data/clean_data/clean_data.csv\"\n",
    "WVS_questions = \"../data_set/cultural_data/clean_data/WVS_topics_questions2.csv\"\n",
    "country_name = \"../data_set/cultural_data/clean_data/country_code.csv\"\n",
    "religion_name = \"../data_set/cultural_data/clean_data/religion_code.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(raw_data,header=None)\n",
    "questions = pd.read_csv(WVS_questions,header=None)\n",
    "country = pd.read_csv(country_name,header=None)\n",
    "religion = pd.read_csv(religion_name,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add headers to dataframes of questions, country, religion and add the title of questions as headers to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.columns = [\"title\",\"min\",\"max\",\"type\",\"norm_type\",\"text\"]\n",
    "data.columns = questions[\"title\"]\n",
    "country.columns = [\"name\"]\n",
    "religion.columns = [\"data\",\"name\",\"type\"]\n",
    "\n",
    "data.to_csv(\"../data_set/cultural_data/clean_data/data_0.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Country preprocessing & selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing country names\n",
    "new = country['name'].str.split(\"'\", n = 1, expand = True)\n",
    "new[1] = new[1].str.replace(\"'\",\"\")\n",
    "new[0] = new[0].astype(int)\n",
    "country['index'], country['name'] = new[0],new[1]\n",
    "country.to_csv(\"../data_set/cultural_data/clean_data/country.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all the representative countries of each cultural group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72751\n"
     ]
    }
   ],
   "source": [
    "rep_country_name = \"../data_set/cultural_data/clean_data/representative_countries.csv\"\n",
    "rep_country = pd.read_csv(rep_country_name,header=None)\n",
    "rep_country.columns = [\"country\",\"culture_group\"]\n",
    "\n",
    "index_df = rep_country['country'].str.split(\" \", n = 1, expand = True)\n",
    "rep_country[\"index\"],rep_country[\"country\"] = index_df[0],index_df[1]\n",
    "\n",
    "data = data[data[\"V2\"].isin(rep_country[\"index\"])]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Work with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are meaningless or not applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "columns_name = \"../data_set/cultural_data/clean_data/columns.csv\"\n",
    "columns_df = pd.read_csv(columns_name,header=None)\n",
    "columns_df.columns = [\"index\",\"question\",\"operation_type\",\"question_type\"]\n",
    "\n",
    "dropped_col = columns_df[columns_df[\"operation_type\"]==0][\"index\"].to_list()\n",
    "\n",
    "data = data.drop(columns=dropped_col)\n",
    "print(len(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impute missing values with country means and output countries with all empty values to rm_country2.csv  \n",
    "(Because if we impute by country means, we would end up with fractions when we convert categorical values to numeric, therefore we choose to impute by country mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "V2          0\n",
      "V4          0\n",
      "V5          0\n",
      "V6          0\n",
      "V7          0\n",
      "        ...  \n",
      "V246     6604\n",
      "V248        0\n",
      "V249        0\n",
      "V250     1000\n",
      "V253    14364\n",
      "Length: 224, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data[data<0] = np.nan\n",
    "gb = data.groupby(\"V2\")\n",
    "dfs = [[x,gb.get_group(x)] for x in gb.groups]\n",
    "new_dfs = []\n",
    "rm_country = pd.DataFrame(columns=[\"country_code\",\"country_name\",\"len_empty\",\"empty_column\"])\n",
    "\n",
    "# For each country, if any column has all NA value, append the column & country name to rm_country, else impute the NA value with country mean\n",
    "for ele in dfs:\n",
    "    cur_df = ele[1]\n",
    "    empty_column = []   \n",
    "    \n",
    "    for column in cur_df.columns:\n",
    "        if cur_df[column].isnull().sum() == len(cur_df[column]):\n",
    "            empty_column.append(column)\n",
    "        else:\n",
    "            cur_df[column].fillna(cur_df[column].mode()[0], inplace = True)\n",
    "#             cur_df[column].fillna(int(cur_df[column].mean()), inplace = True)\n",
    "    \n",
    "    c_name = country[country['index'] == ele[0]]['name'].iloc[0]\n",
    "    rm_country.loc[len(rm_country)] = [ele[0],c_name,len(empty_column),empty_column]\n",
    "    new_dfs.append(cur_df)\n",
    "\n",
    "data = pd.concat(new_dfs)\n",
    "data.to_csv(\"../data_set/cultural_data/clean_data/data_1.csv\", index = False)\n",
    "print(data.isnull().sum())\n",
    "rm_country.to_csv(\"../data_set/cultural_data/clean_data/rm_country2.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing value with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V36', 'V37', 'V38', 'V40', 'V41', 'V42', 'V43', 'V95', 'V178', 'V185', 'V203A', 'V204', 'V206', 'V207A', 'V245', 'V246', 'V253']\n",
      "V36 0\n",
      "V37 0\n",
      "V38 0\n",
      "V40 0\n",
      "V41 0\n",
      "V42 0\n",
      "V43 0\n",
      "V95 0\n",
      "V178 0\n",
      "V185 0\n",
      "V203A 0\n",
      "V204 0\n",
      "V206 0\n",
      "V207A 0\n",
      "V245 0\n",
      "V246 0\n",
      "V253 0\n"
     ]
    }
   ],
   "source": [
    "add_col = columns_df[columns_df[\"operation_type\"]==2][\"index\"].to_list()\n",
    "print(add_col)\n",
    "for col in add_col:\n",
    "    if data[col].min == 0:\n",
    "        data[col] = data[col] + 1\n",
    "    data.loc[data[col].isnull(), col] = 0\n",
    "    print(col,data[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['V24', 'V56'], ['V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35'], ['V39', 'V46'], ['V45', 'V50', 'V54'], ['V51', 'V53'], ['V74', 'V74B'], ['V82', 'V83'], ['V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94'], ['V109', 'V129', 'V135'], ['V110', 'V111', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167'], ['V157', 'V160', 'V168'], ['V226', 'V227', 'V228'], ['V229', 'V235'], ['V198', 'V199', 'V201'], ['V243', 'V244']]\n"
     ]
    }
   ],
   "source": [
    "com_cols = [columns_df[columns_df[\"operation_type\"] == x][\"index\"].to_list() for x in range(3,18)]\n",
    "print(com_cols)\n",
    "\n",
    "for com_col in com_cols:\n",
    "    com_col = list(set(com_col)-(set(com_col)-set(data.columns)))\n",
    "    \n",
    "    # Add missing value for each column\n",
    "    for col in com_col:\n",
    "        if data[col].min == 0:\n",
    "            data[col] = data[col] + 1\n",
    "        data.loc[data[col].isnull(), col] = 0\n",
    "    \n",
    "    # Combine different columns with their sum\n",
    "    data[com_col[0]] = data[com_col].sum(axis=1)\n",
    "    com_col.remove(com_col[0])\n",
    "    data = data.drop(columns=com_col)\n",
    "\n",
    "# output counts for missing value in all columns\n",
    "data.isnull().sum().to_csv(\"../data_set/cultural_data/clean_data/empty_columns.csv\", index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with columns which have special meanings for NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Column V58: replace Hong Kong NA with 0.6\n",
    "data[\"V58\"].fillna(0.6, inplace = True)\n",
    "\n",
    "# Column V144, V146, V147, V148, V149, V152, V154: replace Egypt NA with muslim/often/yes\n",
    "data[\"V144\"].fillna(5, inplace = True)   # Muslim 5\n",
    "data[\"V146\"].fillna(1, inplace = True)   # often 1\n",
    "\n",
    "col_ls = [\"V147\",\"V148\",\"V149\",\"V152\",\"V154\"]\n",
    "for col in col_ls:\n",
    "    data[col].fillna(1, inplace = True)   # yes 1\n",
    "\n",
    "# Column V203 homosexuality: replace Egypt NA with never\n",
    "data[\"V203\"].fillna(1, inplace = True)\n",
    "\n",
    "# Column V250 live with parents: replace Hong Kong NA with yes\n",
    "data[\"V250\"].fillna(1, inplace = True)\n",
    "\n",
    "print(data.isnull().sum().sum())\n",
    "data.to_csv(\"../data_set/cultural_data/clean_data/data_1.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Reduce column cardinality (religion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the religion dataset, output the cleaned religion and convert the religion column in data to categorical codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index                               name  type_n\n",
      "0         0                               None       0\n",
      "1         1                          Aglipayan       4\n",
      "2         2                           Al-Hadis       5\n",
      "3         3                           Alliance       4\n",
      "4         4  Ancestral worshipping / Tradition      10\n",
      "..      ...                                ...     ...\n",
      "97   710002   ZA: African Traditional Religion      10\n",
      "98       -5                                 Na      -1\n",
      "99       -4                                 Na      -1\n",
      "100      -2                                 Na      -1\n",
      "101      -1                                 Na      -1\n",
      "\n",
      "[102 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_religion = religion[4:].reset_index()\n",
    "\n",
    "religion_df = pd.DataFrame(columns=[\"index\",\"name\",\"type_n\"])\n",
    "religion_data = cleaned_religion[\"data\"].str.split(\"'\", n = 1, expand = True)\n",
    "religion_data[1] = religion_data[1].str.replace(\"'\",\"\")\n",
    "religion_data[0] = religion_data[0].astype(int)\n",
    "\n",
    "religion_map = {'Non-religious':0,'Spiritualism':1,'General':2,'Judaism':3,'Christianity':4,'Islam':5,\n",
    "                'Syncretism':6,'Confucianism':7,'Taoism':8,'Buddhism':9,'Ethnic':10,\n",
    "                'Jainism':11,'Paganism':12,'Zoroastrianism':13,np.nan:-1}\n",
    "religion_df[\"index\"],religion_df[\"name\"] = religion_data[0],religion_data[1]\n",
    "religion_df[\"type_n\"] = [religion_map[x] for x in cleaned_religion[\"type\"]]\n",
    "\n",
    "new = pd.DataFrame({\"index\":[-5,-4,-2,-1],\"name\":[\"Na\"]*4,\"type_n\":[-1]*4})\n",
    "religion_df = pd.concat([religion_df,new]).reset_index().drop(columns=[\"level_0\"])\n",
    "print(religion_df)\n",
    "religion_df.to_csv(\"../data_set/cultural_data/clean_data/cleaned_religion.csv\", index = False)\n",
    "\n",
    "data[\"V144\"] = [religion_df[religion_df[\"index\"]==x][\"type_n\"].iloc[0] for x in data[\"V144\"]]\n",
    "\n",
    "# print(data[\"V144\"])\n",
    "data.to_csv(\"../data_set/cultural_data/clean_data/data_2.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Convert categorical into numeric & normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process country dataframe and output the cleaned country dataset and the respondent per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = data.groupby([\"V2\"]).count().sort_values([\"V4\"],ascending=False)\n",
    "\n",
    "country_df = pd.DataFrame(columns=[\"name\",\"index\",\"respondents\"])\n",
    "country_df[\"index\"] = country_data.index\n",
    "country_df[\"respondents\"] = np.array(country_data['V4'])\n",
    "\n",
    "country_df[\"name\"] = [country[country['index'] == x]['name'].iloc[0] for x in country_df[\"index\"]]\n",
    "\n",
    "country_df.to_csv(\"../data_set/cultural_data/clean_data/country_respondent.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "columns = data.columns.to_list()\n",
    "columns.remove(\"V2\")\n",
    "\n",
    "for column in columns:\n",
    "    minimum = int(data[column].min())\n",
    "    maximum = int(data[column].max())\n",
    "    norm_type = questions[questions[\"title\"]==column].norm_type.iloc[0]\n",
    "    \n",
    "    if norm_type == 2:\n",
    "        for i in range(minimum,maximum+1):\n",
    "            index = data.columns.get_loc(column) + i\n",
    "            name = column+\"_\"+str(i)\n",
    "            col = [0]*len(data[column])\n",
    "            for j in range(len(data[column])):\n",
    "                col[j] = 1 if data[column].iloc[j] == i else 0\n",
    "            data.insert(index,name,col)\n",
    "        data = data.drop(columns=[column])\n",
    "\n",
    "    elif norm_type == 1:\n",
    "        data[column] = (data[column]-minimum)/(maximum-minimum)\n",
    "        \n",
    "    elif norm_type == 3:\n",
    "        data[column] = 1-(data[column]-minimum)/(maximum-minimum)\n",
    "        \n",
    "data.to_csv(\"../data_set/cultural_data/clean_data/data_3.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
